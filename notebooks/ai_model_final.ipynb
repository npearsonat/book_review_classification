{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae59537-031a-4077-a108-0846d4914d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c84d61b2-63b1-4190-8552-7f3fff89fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.read_csv('data/engineered_features.csv')\n",
    "df = pd.read_csv('data/data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22c3e608-7299-4159-9077-49d529e73a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Processing Functions\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning - nothing fancy\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    # Keep letters, numbers, and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    # Clean up extra spaces\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def combine_text_fields(df):\n",
    "    \"\"\"Combine review text and summary into one field\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    if 'combined_text' not in data.columns:\n",
    "        # Start with empty strings\n",
    "        data['combined_text'] = ''\n",
    "        \n",
    "        # Add review text if it exists\n",
    "        if 'review/text' in data.columns:\n",
    "            data['combined_text'] += data['review/text'].fillna('')\n",
    "        \n",
    "        # Add review summary if it exists\n",
    "        if 'review/summary' in data.columns:\n",
    "            # Add a space between text and summary if both exist\n",
    "            data['combined_text'] += ' ' + data['review/summary'].fillna('')\n",
    "        \n",
    "        # Clean up any extra spaces\n",
    "        data['combined_text'] = data['combined_text'].str.strip()\n",
    "    \n",
    "    data['clean_text'] = data['combined_text'].apply(clean_text)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0767eb2-e76e-45c7-8383-85f37e87c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Feature Engineering Functions\n",
    "\n",
    "def extract_basic_stats(df):\n",
    "    \"\"\"Pull out basic text statistics\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    data['char_count'] = data['clean_text'].str.len()\n",
    "    data['word_count'] = data['clean_text'].str.split().str.len().fillna(0)\n",
    "    \n",
    "    def avg_word_len(text):\n",
    "        words = text.split() if text else []\n",
    "        return np.mean([len(w) for w in words]) if words else 0\n",
    "    \n",
    "    data['avg_word_len'] = data['clean_text'].apply(avg_word_len)\n",
    "    data['sentence_count'] = data['clean_text'].str.count(r'[.!?]') + 1\n",
    "    data['words_per_sentence'] = data['word_count'] / data['sentence_count']\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_sentiment_signals(df):\n",
    "    \"\"\"Look for positive and negative sentiment indicators\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    good_words = ['good', 'great', 'excellent', 'amazing', 'love', 'perfect', \n",
    "                 'wonderful', 'fantastic', 'outstanding', 'recommend']\n",
    "    bad_words = ['bad', 'terrible', 'awful', 'hate', 'horrible', 'worst', \n",
    "                'disappointing', 'waste', 'useless', 'poor']\n",
    "    \n",
    "    def count_sentiment(text, word_list):\n",
    "        words = text.split() if text else []\n",
    "        matches = sum(1 for word in words if word in word_list)\n",
    "        return matches / max(len(words), 1)\n",
    "    \n",
    "    data['positive_signal'] = data['clean_text'].apply(lambda x: count_sentiment(x, good_words))\n",
    "    data['negative_signal'] = data['clean_text'].apply(lambda x: count_sentiment(x, bad_words))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_style_features(df):\n",
    "    \"\"\"Extract writing style indicators\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    def caps_ratio(text):\n",
    "        if not text:\n",
    "            return 0\n",
    "        return sum(1 for c in str(text) if c.isupper()) / max(len(str(text)), 1)\n",
    "    \n",
    "    data['caps_ratio'] = data['combined_text'].apply(caps_ratio)\n",
    "    data['punct_ratio'] = data['combined_text'].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c in '!?.,;:') / max(len(str(x)), 1)\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def build_all_features(df):\n",
    "    \"\"\"Combine all feature extraction steps\"\"\"\n",
    "    data = combine_text_fields(df)\n",
    "    data = extract_basic_stats(data)\n",
    "    data = extract_sentiment_signals(data)\n",
    "    data = extract_style_features(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bdeefe-33f9-46ed-ba54-a34d5f7f8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Functions\n",
    "\n",
    "def prepare_text_features(df, vectorizer=None, vocab_size=10000):\n",
    "    \"\"\"Convert text to numerical vectors using TF-IDF\"\"\"\n",
    "    if vectorizer is None:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=vocab_size,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=3,\n",
    "            max_df=0.9,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        text_vectors = vectorizer.fit_transform(df['clean_text'])\n",
    "        return text_vectors, vectorizer\n",
    "    else:\n",
    "        text_vectors = vectorizer.transform(df['clean_text'])\n",
    "        return text_vectors, vectorizer\n",
    "\n",
    "\n",
    "def prepare_numeric_features(df, scaler=None):\n",
    "    \"\"\"Scale and prepare numerical features\"\"\"\n",
    "    feature_cols = [\n",
    "        'char_count', 'word_count', 'avg_word_len', \n",
    "        'sentence_count', 'words_per_sentence',\n",
    "        'positive_signal', 'negative_signal', \n",
    "        'caps_ratio', 'punct_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Include helpfulness score if available\n",
    "    if 'review/helpfulness' in df.columns:\n",
    "        feature_cols.append('review/helpfulness')\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_data = df[feature_cols].copy()\n",
    "    for col in feature_cols:\n",
    "        if col in numeric_data.columns:\n",
    "            numeric_data[col] = numeric_data[col].fillna(numeric_data[col].median())\n",
    "    \n",
    "    # Scale features\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(numeric_data)\n",
    "        return scaled_features, scaler\n",
    "    else:\n",
    "        scaled_features = scaler.transform(numeric_data)\n",
    "        return scaled_features, scaler\n",
    "\n",
    "\n",
    "def combine_features(text_vectors, numeric_features):\n",
    "    \"\"\"Combine text and numeric features into one matrix\"\"\"\n",
    "    from scipy.sparse import hstack, csr_matrix\n",
    "    return hstack([text_vectors, csr_matrix(numeric_features)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851a7d75-e59e-4711-a6d8-b522e4d93426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Classifier Class\n",
    "\n",
    "class TextAnalyzer:\n",
    "    \"\"\"\n",
    "    A text classifier that learns to identify helpful reviews\n",
    "    Uses neural networks under the hood but keeps things simple\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers=None, vocab_size=12000):\n",
    "        self.layers = layers or (384, 192, 96)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.brain = None\n",
    "        self.text_processor = None\n",
    "        self.feature_scaler = None\n",
    "        self.test_data = None\n",
    "        \n",
    "    def train(self, df, target='is_helpful', validation_split=0.2, training_rounds=50):\n",
    "        \"\"\"Train the classifier on the data\"\"\"\n",
    "        print(\"Training text classifier...\")\n",
    "        print(f\"Working with {len(df)} reviews\")\n",
    "        \n",
    "        # Build features\n",
    "        processed = build_all_features(df)\n",
    "        \n",
    "        # Check target distribution\n",
    "        target_counts = processed[target].value_counts()\n",
    "        print(f\"Target distribution: {dict(target_counts)}\")\n",
    "        \n",
    "        # Prepare text features\n",
    "        text_vectors, self.text_processor = prepare_text_features(\n",
    "            processed, vocab_size=self.vocab_size\n",
    "        )\n",
    "        \n",
    "        # Prepare numeric features\n",
    "        numeric_features, self.feature_scaler = prepare_numeric_features(processed)\n",
    "        \n",
    "        # Combine all features\n",
    "        X = combine_features(text_vectors, numeric_features)\n",
    "        y = processed[target]\n",
    "        \n",
    "        print(f\"Using {X.shape[1]} total features\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training on {X_train.shape[0]} samples, testing on {X_test.shape[0]}\")\n",
    "        \n",
    "        # Create and train neural network\n",
    "        self.brain = MLPClassifier(\n",
    "            hidden_layer_sizes=self.layers,\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=0.0001,\n",
    "            learning_rate_init=0.001,\n",
    "            max_iter=training_rounds,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.15,\n",
    "            n_iter_no_change=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Training neural network with layers: {self.layers}\")\n",
    "        self.brain.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        self._evaluate_training(X_train, X_test, y_train, y_test)\n",
    "        self.test_data = (X_test, y_test)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _evaluate_training(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Internal method to evaluate training results\"\"\"\n",
    "        train_pred = self.brain.predict(X_train)\n",
    "        test_pred = self.brain.predict(X_test)\n",
    "        train_proba = self.brain.predict_proba(X_train)[:, 1]\n",
    "        test_proba = self.brain.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        train_auc = roc_auc_score(y_train, train_proba)\n",
    "        test_auc = roc_auc_score(y_test, test_proba)\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"Training accuracy: {train_acc:.3f}\")\n",
    "        print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "        print(f\"Training AUC: {train_auc:.3f}\")\n",
    "        print(f\"Test AUC: {test_auc:.3f}\")\n",
    "    \n",
    "    def predict(self, input_data, with_confidence=True):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        if self.brain is None:\n",
    "            raise ValueError(\"Need to train the model first!\")\n",
    "        \n",
    "        # Handle different input types\n",
    "        if isinstance(input_data, str):\n",
    "            temp_df = pd.DataFrame({'combined_text': [input_data]})\n",
    "        elif isinstance(input_data, list):\n",
    "            temp_df = pd.DataFrame({'combined_text': input_data})\n",
    "        else:\n",
    "            temp_df = input_data\n",
    "        \n",
    "        # Process features\n",
    "        processed = build_all_features(temp_df)\n",
    "        text_vectors, _ = prepare_text_features(processed, self.text_processor)\n",
    "        numeric_features, _ = prepare_numeric_features(processed, self.feature_scaler)\n",
    "        X = combine_features(text_vectors, numeric_features)\n",
    "        \n",
    "        predictions = self.brain.predict(X)\n",
    "        \n",
    "        if with_confidence:\n",
    "            probabilities = self.brain.predict_proba(X)[:, 1]\n",
    "            return predictions, probabilities\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def check_review(self, text):\n",
    "        \"\"\"Analyze a single review\"\"\"\n",
    "        pred, prob = self.predict([text])\n",
    "        \n",
    "        return {\n",
    "            'text_preview': text[:150] + \"...\" if len(text) > 150 else text,\n",
    "            'helpful': bool(pred[0]),\n",
    "            'confidence': float(prob[0]),\n",
    "            'prediction': 'Helpful' if pred[0] else 'Not Helpful'\n",
    "        }\n",
    "    \n",
    "    def analyze_performance(self):\n",
    "        \"\"\"Detailed performance analysis\"\"\"\n",
    "        if not self.test_data:\n",
    "            print(\"No test data available - train the model first\")\n",
    "            return\n",
    "        \n",
    "        X_test, y_test = self.test_data\n",
    "        predictions = self.brain.predict(X_test)\n",
    "        probabilities = self.brain.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        auc = roc_auc_score(y_test, probabilities)\n",
    "        \n",
    "        print(f\"Model Performance:\")\n",
    "        print(f\"Accuracy: {acc:.3f}\")\n",
    "        print(f\"AUC Score: {auc:.3f}\")\n",
    "        print(f\"Trained for {self.brain.n_iter_} iterations\")\n",
    "        \n",
    "        print(\"\\nDetailed breakdown:\")\n",
    "        print(classification_report(y_test, predictions, \n",
    "                                  target_names=['Not Helpful', 'Helpful']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc582d91-f407-449f-bac0-e5836b4ba3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organic Text Classifier\n",
      "Usage: classifier = build_classifier(your_dataframe)\n",
      "Then: result = classifier.check_review('your text here')\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions\n",
    "\n",
    "\n",
    "def build_classifier(data, hidden_layers=(256, 128), vocab_size=10000):\n",
    "    \"\"\"Build and train a text classifier\"\"\"\n",
    "    classifier = TextAnalyzer(layers=hidden_layers, vocab_size=vocab_size)\n",
    "    classifier.train(data, training_rounds=100)\n",
    "    classifier.analyze_performance()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def demo_predictions(classifier, data, num_examples=3):\n",
    "    \"\"\"Show some example predictions\"\"\"\n",
    "    print(\"\\nExample predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    samples = data.sample(n=min(num_examples, len(data)))\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        text = row.get('review/text', row.get('combined_text', ''))\n",
    "        actual = row['is_helpful']\n",
    "        \n",
    "        result = classifier.check_review(text)\n",
    "        \n",
    "        print(f\"Text: {result['text_preview']}\")\n",
    "        print(f\"Actual: {'Helpful' if actual else 'Not Helpful'}\")\n",
    "        print(f\"Predicted: {result['prediction']} ({result['confidence']:.3f})\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Quick start\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Organic Text Classifier\")\n",
    "    print(\"Usage: classifier = build_classifier(your_dataframe)\")\n",
    "    print(\"Then: result = classifier.check_review('your text here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d3987a9-d977-4f7c-9377-daa35ad73ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text classifier...\n",
      "Working with 13477 reviews\n",
      "Target distribution: {0: np.int64(6879), 1: np.int64(6598)}\n",
      "Using 10010 total features\n",
      "Training on 10781 samples, testing on 2696\n",
      "Training neural network with layers: (256, 128)\n",
      "\n",
      "Results:\n",
      "Training accuracy: 0.993\n",
      "Test accuracy: 0.961\n",
      "Training AUC: 1.000\n",
      "Test AUC: 0.995\n",
      "Model Performance:\n",
      "Accuracy: 0.961\n",
      "AUC Score: 0.995\n",
      "Trained for 13 iterations\n",
      "\n",
      "Detailed breakdown:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Helpful       0.97      0.95      0.96      1376\n",
      "     Helpful       0.95      0.97      0.96      1320\n",
      "\n",
      "    accuracy                           0.96      2696\n",
      "   macro avg       0.96      0.96      0.96      2696\n",
      "weighted avg       0.96      0.96      0.96      2696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = build_classifier(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
